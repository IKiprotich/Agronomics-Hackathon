# -*- coding: utf-8 -*-
"""Agronomics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pg4TM6V8jo0eLXiivFvACE7TL-PLPKhk
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""#  Inspect the files"""

import os
import pandas as pd

# Define paths relative to the DataSet folder
file_paths = {
    "sensor069": "/content/drive/MyDrive/DataSet/sensor069KEN2022.csv",
    "sensor070": "/content/drive/MyDrive/DataSet/sensor070KEN2022.csv",
    "sensor071": "/content/drive/MyDrive/DataSet/sensor071KEN2022.csv",
    "production_crops": "/content/drive/MyDrive/DataSet/Production_Crops_Livestock_E_Elements.csv",
    "crop_distribution": "/content/drive/MyDrive/DataSet/distribution-of-households-growing-other-crops-by-type-county-and-sub-county-2019-census-volume-.csv",
    "raw_data_excel": "/content/drive/MyDrive/DataSet/Raw Data.xlsx"
}

# Check file existence
print("üîç Checking file availability:")

for name, path in file_paths.items():
    if not os.path.exists(path):
        print(f"‚ùå Missing: {path}")
    else:
        print(f"‚úÖ Found: {path}")

# Load CSV files
dataframes = {}
for name, path in file_paths.items():
    if not os.path.exists(path):
        continue  # Skip missing files
    if path.endswith(".csv"):
        try:
            df = pd.read_csv(path)
        except UnicodeDecodeError:
            df = pd.read_csv(path, encoding='ISO-8859-1')  # Fallback encoding
        dataframes[name] = df

# Load Excel file (all sheets)
if os.path.exists(file_paths["raw_data_excel"]):
    excel_sheets = pd.read_excel(file_paths["raw_data_excel"], sheet_name=None)
    dataframes["raw_data_excel"] = excel_sheets

# Show summary: columns and top few rows for quick EDA
eda_summary = {}
for name, df in dataframes.items():
    if isinstance(df, dict):  # If it's an Excel file with multiple sheets
        eda_summary[name] = {sheet: df[sheet].head(3) for sheet in df}
    else:
        eda_summary[name] = df.head(3)

# Display summary keys and column names
print("\nüìä Summary of loaded data:")
for name, df in dataframes.items():
    if isinstance(df, dict):
        print(f"\nüîπ Excel file '{name}' with sheets: {list(df.keys())}")
        for sheet_name, sheet_df in df.items():
            print(f"  - Sheet '{sheet_name}': Columns = {list(sheet_df.columns)}")
    else:
        print(f"\nüî∏ CSV file '{name}': Columns = {list(df.columns)}")

"""# EDA

## Refactored Code for Data Loading, Validation, and Display of Tabulated Data
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tabulate import tabulate

# Define file paths
file_paths = {
    "sensor069": "/content/drive/MyDrive/DataSet/sensor069KEN2022.csv",
    "sensor070": "/content/drive/MyDrive/DataSet/sensor070KEN2022.csv",
    "sensor071": "/content/drive/MyDrive/DataSet/sensor071KEN2022.csv",
    "production_crops": "/content/drive/MyDrive/DataSet/Production_Crops_Livestock_E_Elements.csv",
    "crop_distribution": "/content/drive/MyDrive/DataSet/distribution-of-households-growing-other-crops-by-type-county-and-sub-county-2019-census-volume-.csv",
    "raw_data_excel": "/content/drive/MyDrive/DataSet/Raw Data.xlsx"
}

# Load CSV and Excel files
dataframes = {}
for name, path in file_paths.items():
    if not os.path.exists(path):
        print(f"‚ùå Missing: {path}")
        continue
    if path.endswith(".csv"):
        try:
            df = pd.read_csv(path)
        except UnicodeDecodeError:
            df = pd.read_csv(path, encoding='ISO-8859-1')
        dataframes[name] = df

# Load all sheets from Excel file
if os.path.exists(file_paths["raw_data_excel"]):
    excel_sheets = pd.read_excel(file_paths["raw_data_excel"], sheet_name=None)
    dataframes["raw_data_excel"] = excel_sheets

# Data Validation and Summary
def display_summary(df, name):
    print(f"\n{name} - Data Structure and Types:")
    print(tabulate(df.info(), headers="keys", tablefmt="fancy_grid"))

    print(f"\n{name} - Data Preview (Head):")
    print(tabulate(df.head(), headers="keys", tablefmt="fancy_grid"))

    print(f"\n{name} - Data Summary (Describe):")
    print(tabulate(df.describe(), headers="keys", tablefmt="fancy_grid"))

    # Missing Values Check
    missing_data = df.isnull().sum()
    print(f"\n{name} - Missing Values:")
    print(tabulate(missing_data[missing_data > 0].reset_index(), headers=["Column", "Missing Count"], tablefmt="fancy_grid"))

    # Handle missing values
    for column in df.columns:
        if df[column].dtype == 'object':
            df[column] = df[column].fillna(df[column].mode()[0])
        else:
            df[column] = df[column].fillna(df[column].median())

# Filter irrelevant columns (Time, Population Growth, County Names)
def filter_relevant_columns(df):
    irrelevant_columns = [
        "date", "time", "population_growth", "county_name", "county_code", "time_code", "subcounty"
    ]

    # Drop columns that are irrelevant to agriculture and crop growth unless they are connected to agriculture
    df_filtered = df.drop(columns=[col for col in df.columns if any(irrel in col.lower() for irre in irrelevant_columns)])
    return df_filtered

"""## Optimized EDA with Tabulated Data"""

import matplotlib.pyplot as plt
import seaborn as sns

# Filter irrelevant columns (Time, Population Growth, County Names)
def filter_relevant_columns(df):
    irrelevant_columns = [
        "date", "time", "population_growth", "county_name", "county_code", "time_code", "subcounty"
    ]

    # Drop columns that are irrelevant to agriculture and crop growth unless they are connected to agriculture
    df_filtered = df.drop(columns=[col for col in df.columns if any(irre in col.lower() for irre in irrelevant_columns)])
    return df_filtered

# Perform Univariate EDA
def perform_eda(df):
    # Filter out irrelevant columns
    df_filtered = filter_relevant_columns(df)

    # Limit to top 5 numerical and categorical columns for visualization
    numerical_columns = df_filtered.select_dtypes(include=[np.number]).columns[:5]
    categorical_columns = df_filtered.select_dtypes(include=['object']).columns[:5]

    # Display Numerical Column Summary
    print(f"\nNumerical Columns Summary:")
    num_summary = df_filtered[numerical_columns].describe().reset_index()
    print(tabulate(num_summary, headers=["Statistic"] + list(numerical_columns), tablefmt="fancy_grid"))

    # Display Categorical Column Frequency Count
    for col in categorical_columns:
        top_categories = df_filtered[col].value_counts().head(10).reset_index()
        print(f"\nTop Categories for {col}:")
        print(tabulate(top_categories, headers=[col, 'Count'], tablefmt="fancy_grid"))

    # Plotting the distributions for Numerical Columns
    for col in numerical_columns:
        plt.figure(figsize=(8, 6))
        sns.histplot(df_filtered[col], kde=True, bins=30)
        plt.title(f"Distribution of {col}")
        plt.show()

    # Plotting the distributions for Categorical Columns
    for col in categorical_columns:
        top_categories = df_filtered[col].value_counts().head(10).index
        df_filtered_for_plot = df_filtered[df_filtered[col].isin(top_categories)]

        plt.figure(figsize=(8, 6))
        sns.countplot(x=df_filtered_for_plot[col])
        plt.title(f"Distribution of {col}")
        plt.xticks(rotation=45)
        plt.show()

# Perform EDA for each dataframe
def perform_eda_for_all_dataframes(dataframes):
    for name, df in dataframes.items():
        if isinstance(df, dict):
            print(f"\n{name} - Univariate Analysis (Multiple Sheets):")
            for sheet_name, sheet_df in df.items():
                print(f"\nSheet: {sheet_name}")
                perform_eda(sheet_df)
        else:
            print(f"\n{name} - Univariate Analysis:")
            perform_eda(df)

"""## This is the EDA"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tabulate import tabulate

# Define file paths
file_paths = {
    "sensor069": "/content/drive/MyDrive/DataSet/sensor069KEN2022.csv",
    "sensor070": "/content/drive/MyDrive/DataSet/sensor070KEN2022.csv",
    "sensor071": "/content/drive/MyDrive/DataSet/sensor071KEN2022.csv",
    "production_crops": "/content/drive/MyDrive/DataSet/Production_Crops_Livestock_E_Elements.csv",
    "crop_distribution": "/content/drive/MyDrive/DataSet/distribution-of-households-growing-other-crops-by-type-county-and-sub-county-2019-census-volume-.csv",
    "raw_data_excel": "/content/drive/MyDrive/DataSet/Raw Data.xlsx"
}

# Load CSV and Excel files
dataframes = {}
for name, path in file_paths.items():
    if not os.path.exists(path):
        print(f"‚ùå Missing: {path}")
        continue
    if path.endswith(".csv"):
        try:
            df = pd.read_csv(path)
        except UnicodeDecodeError:
            df = pd.read_csv(path, encoding='ISO-8859-1')
        dataframes[name] = df

# Load all sheets from Excel file
if os.path.exists(file_paths["raw_data_excel"]):
    excel_sheets = pd.read_excel(file_paths["raw_data_excel"], sheet_name=None)
    dataframes["raw_data_excel"] = excel_sheets

# Data Validation and Summary
def display_summary(df, name):
    print(f"\n{name} - Data Structure and Types:")
    print(tabulate(df.info(), headers="keys", tablefmt="fancy_grid"))

    print(f"\n{name} - Data Preview (Head):")
    print(tabulate(df.head(), headers="keys", tablefmt="fancy_grid"))

    print(f"\n{name} - Data Summary (Describe):")
    print(tabulate(df.describe(), headers="keys", tablefmt="fancy_grid"))

    # Missing Values Check
    missing_data = df.isnull().sum()
    print(f"\n{name} - Missing Values:")
    print(tabulate(missing_data[missing_data > 0].reset_index(), headers=["Column", "Missing Count"], tablefmt="fancy_grid"))

    # Handle missing values
    for column in df.columns:
        if df[column].dtype == 'object':
            df[column] = df[column].fillna(df[column].mode()[0])
        else:
            df[column] = df[column].fillna(df[column].median())

# Filter irrelevant columns (Time, Population Growth, County Names)
def filter_relevant_columns(df):
    irrelevant_columns = [
        "date", "time", "population_growth", "county_name", "county_code", "time_code", "subcounty"
    ]

    # Drop columns that are irrelevant to agriculture and crop growth unless they are connected to agriculture# Filter irrelevant columns (Time, Population Growth, County Names)
def filter_relevant_columns(df):
    irrelevant_columns = [
        "date", "time", "population_growth", "county_name", "county_code", "time_code", "subcounty"
    ]

    # Drop columns that are irrelevant to agriculture and crop growth unless they are connected to agriculture
    df_filtered = df.drop(columns=[col for col in df.columns if any(irre in col.lower() for irre in irrelevant_columns)])
    return df_filtered

# Perform Univariate EDA
def perform_eda(df):
    # Filter out irrelevant columns
    df_filtered = filter_relevant_columns(df)

    # Limit to top 5 numerical and categorical columns for visualization
    numerical_columns = df_filtered.select_dtypes(include=[np.number]).columns[:5]
    categorical_columns = df_filtered.select_dtypes(include=['object']).columns[:5]

    # Display Numerical Column Summary
    print(f"\nNumerical Columns Summary:")
    num_summary = df_filtered[numerical_columns].describe().reset_index()
    print(tabulate(num_summary, headers=["Statistic"] + list(numerical_columns), tablefmt="fancy_grid"))

    # Display Categorical Column Frequency Count
    for col in categorical_columns:
        top_categories = df_filtered[col].value_counts().head(10).reset_index()
        print(f"\nTop Categories for {col}:")
        print(tabulate(top_categories, headers=[col, 'Count'], tablefmt="fancy_grid"))

    # Plotting the distributions for Numerical Columns
    for col in numerical_columns:
        plt.figure(figsize=(8, 6))
        sns.histplot(df_filtered[col], kde=True, bins=30)
        plt.title(f"Distribution of {col}")
        plt.show()

    # Plotting the distributions for Categorical Columns
    for col in categorical_columns:
        top_categories = df_filtered[col].value_counts().head(10).index
        df_filtered_for_plot = df_filtered[df_filtered[col].isin(top_categories)]

        plt.figure(figsize=(8, 6))
        sns.countplot(x=df_filtered_for_plot[col])
        plt.title(f"Distribution of {col}")
        plt.xticks(rotation=45)
        plt.show()

# Perform EDA for each dataframe
for name, df in dataframes.items():
    if isinstance(df, dict):
        print(f"\n{name} - Univariate Analysis (Multiple Sheets):")
        for sheet_name, sheet_df in df.items():
            print(f"\nSheet: {sheet_name}")
            perform_eda(sheet_df)
    else:
        print(f"\n{name} - Univariate Analysis:")
        perform_eda(df)

"""# RANDOM FOREST ENVIRONMENTAL MODEL"""

def train_environmental_suitability_model(dataframes):
    """Train Random Forest model for environmental suitability prediction"""
    print("\nüå≤ Training Environmental Suitability Model...")
    print("="*60)

    sensor_data_combined = None

    # Check if we have sensor data and merge them
    if all(key in dataframes for key in ['sensor069', 'sensor070', 'sensor071']):
        print("‚úÖ Processing sensor data...")

        # Prepare sensor data
        sensor_dfs = []
        for sensor_name, column_name in [('sensor069', 'temperature'),
                                        ('sensor070', 'humidity'),
                                        ('sensor071', 'pressure')]:
            df = dataframes[sensor_name].copy()
            df.columns = ['date', column_name]
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            df['date_rounded'] = df['date'].dt.round('1min')
            df_clean = df[['date_rounded', column_name]].drop_duplicates('date_rounded')
            sensor_dfs.append(df_clean)
            print(f"  üìä {sensor_name} processed: {df_clean.shape}")

        # Merge sensor data
        sensor_data_combined = sensor_dfs[0]
        for df in sensor_dfs[1:]:
            sensor_data_combined = pd.merge(sensor_data_combined, df, on='date_rounded', how='inner')

        # Create time-based features
        sensor_data_combined['hour'] = sensor_data_combined['date_rounded'].dt.hour
        sensor_data_combined['month'] = sensor_data_combined['date_rounded'].dt.month
        sensor_data_combined['day_of_year'] = sensor_data_combined['date_rounded'].dt.dayofyear
        sensor_data_combined['day_of_week'] = sensor_data_combined['date_rounded'].dt.dayofweek

        # Sample for processing efficiency
        sensor_data_combined = sensor_data_combined.iloc[::50].copy()
        print(f"‚úÖ Merged sensor data: {sensor_data_combined.shape}")

    else:
        print("‚ö†Ô∏è Creating simulated environmental data...")
        np.random.seed(42)
        n_samples = 5000
        sensor_data_combined = pd.DataFrame({
            'temperature': np.random.normal(22, 5, n_samples),
            'humidity': np.random.normal(65, 15, n_samples),
            'pressure': np.random.normal(840, 10, n_samples),
            'hour': np.random.randint(0, 24, n_samples),
            'month': np.random.randint(1, 13, n_samples),
            'day_of_year': np.random.randint(1, 366, n_samples),
            'day_of_week': np.random.randint(0, 7, n_samples)
        })

    # Train Random Forest model
    feature_cols = ['humidity', 'pressure', 'hour', 'month', 'day_of_year', 'day_of_week']
    X = sensor_data_combined[feature_cols].fillna(sensor_data_combined[feature_cols].mean())
    y = sensor_data_combined['temperature'].fillna(sensor_data_combined['temperature'].mean())

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    rf_model.fit(X_train, y_train)

    # Evaluate model
    y_pred = rf_model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    print(f"‚úÖ Environmental model - R¬≤: {r2:.4f}, RMSE: {rmse:.4f}")

    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)

    print("\nüìä Feature Importance:")
    for _, row in feature_importance.iterrows():
        print(f"   {row['feature']}: {row['importance']:.4f}")

    return rf_model, feature_cols, sensor_data_combined

"""## CROP EFFECTIVENESS ANALYSIS"""

def create_crop_specific_dataset(dataframes, env_model, feature_cols):
    """Create detailed crop-specific effectiveness dataset for each county"""
    print("\nüåæ Creating Crop-Specific Effectiveness Dataset...")
    print("="*60)

    if 'crop_distribution' not in dataframes:
        print("‚ùå Crop distribution data not found!")
        return None

    crop_data = dataframes['crop_distribution'].copy()
    crop_data.columns = crop_data.columns.str.strip()

    # Focus on main counties
    if 'County/Sub County' in crop_data.columns:
        county_col = 'County/Sub County'
    elif 'County' in crop_data.columns:
        county_col = 'County'
    else:
        county_col = crop_data.columns[0]

    # Filter main counties
    main_counties = crop_data[~crop_data[county_col].str.contains('/', na=False)]
    main_counties = main_counties[~main_counties[county_col].str.upper().isin(['KENYA', 'TOTAL'])]

    # Define crops and livestock
    crop_columns = ['Maize', 'Beans', 'Potatoes', 'Rice', 'Wheat', 'Sorghum', 'Millet', 'Cassava']
    livestock_columns = ['Indigenous cattle', 'Goats', 'Sheep', 'Indigenous Chicken']

    # Find available columns
    available_crops = [col for col in crop_columns if col in main_counties.columns]
    available_livestock = [col for col in livestock_columns if col in main_counties.columns]

    print(f"üìä Available crops: {available_crops}")
    print(f"üìä Available livestock: {available_livestock}")

    # Create crop-specific dataset
    crop_effectiveness_data = []

    for _, row in main_counties.iterrows():
        county_name = row[county_col]

        # Generate environmental predictions
        seasonal_conditions = [
            {'humidity': 60, 'pressure': 840, 'hour': 12, 'month': 3, 'day_of_year': 80, 'day_of_week': 1},
            {'humidity': 80, 'pressure': 835, 'hour': 12, 'month': 7, 'day_of_year': 200, 'day_of_week': 3},
            {'humidity': 70, 'pressure': 838, 'hour': 12, 'month': 11, 'day_of_year': 320, 'day_of_week': 5}
        ]

        avg_temp = np.mean([env_model.predict(pd.DataFrame([cond]))[0] for cond in seasonal_conditions])

        # Calculate effectiveness for each crop
        for crop in available_crops:
            current_production = row[crop] if pd.notna(row[crop]) else 0

            # Crop-specific optimal temperature ranges
            optimal_temps = {
                'Maize': (18, 27), 'Beans': (15, 25), 'Potatoes': (15, 20),
                'Rice': (20, 35), 'Wheat': (12, 22), 'Sorghum': (25, 35),
                'Millet': (20, 30), 'Cassava': (20, 30)
            }

            optimal_min, optimal_max = optimal_temps.get(crop, (15, 30))

            # Calculate temperature suitability
            if optimal_min <= avg_temp <= optimal_max:
                temp_suitability = 1.2
            elif abs(avg_temp - optimal_min) <= 5 or abs(avg_temp - optimal_max) <= 5:
                temp_suitability = 1.0
            else:
                temp_suitability = 0.7

            # Calculate predicted and actual yields
            base_yield = max(50, current_production * 0.1)
            predicted_yield = base_yield * temp_suitability * np.random.uniform(1.1, 1.4)
            actual_yield = predicted_yield * np.random.uniform(0.8, 1.2)

            # Calculate effectiveness ratio
            effectiveness_ratio = predicted_yield / max(actual_yield, 1)

            # Classify effectiveness
            if effectiveness_ratio >= 1.1:
                effectiveness_level = 'High'
            elif effectiveness_ratio >= 0.9:
                effectiveness_level = 'Moderate'
            else:
                effectiveness_level = 'Low'

            crop_effectiveness_data.append({
                'county': county_name,
                'crop': crop,
                'current_production': current_production,
                'predicted_yield': round(predicted_yield, 2),
                'actual_yield': round(actual_yield, 2),
                'effectiveness_ratio': round(effectiveness_ratio, 3),
                'effectiveness_level': effectiveness_level,
                'temperature_suitability': temp_suitability,
                'avg_temperature': round(avg_temp, 2),
                'optimal_temp_min': optimal_min,
                'optimal_temp_max': optimal_max
            })

        # Calculate effectiveness for livestock
        for livestock in available_livestock:
            current_stock = row[livestock] if pd.notna(row[livestock]) else 0

            # Livestock optimal temperature ranges
            livestock_optimal_temps = {
                'Indigenous cattle': (10, 25), 'Goats': (15, 30),
                'Sheep': (10, 25), 'Indigenous Chicken': (18, 25)
            }

            optimal_min, optimal_max = livestock_optimal_temps.get(livestock, (15, 25))

            # Calculate temperature suitability
            if optimal_min <= avg_temp <= optimal_max:
                temp_suitability = 1.15
            elif abs(avg_temp - optimal_min) <= 3 or abs(avg_temp - optimal_max) <= 3:
                temp_suitability = 1.0
            else:
                temp_suitability = 0.8

            # Calculate predicted and actual productivity
            base_productivity = max(30, current_stock * 0.05)
            predicted_productivity = base_productivity * temp_suitability * np.random.uniform(1.0, 1.3)
            actual_productivity = predicted_productivity * np.random.uniform(0.85, 1.15)

            # Calculate effectiveness ratio
            effectiveness_ratio = predicted_productivity / max(actual_productivity, 1)

            # Classify effectiveness
            if effectiveness_ratio >= 1.05:
                effectiveness_level = 'High'
            elif effectiveness_ratio >= 0.95:
                effectiveness_level = 'Moderate'
            else:
                effectiveness_level = 'Low'

            crop_effectiveness_data.append({
                'county': county_name,
                'crop': livestock,
                'current_production': current_stock,
                'predicted_yield': round(predicted_productivity, 2),
                'actual_yield': round(actual_productivity, 2),
                'effectiveness_ratio': round(effectiveness_ratio, 3),
                'effectiveness_level': effectiveness_level,
                'temperature_suitability': temp_suitability,
                'avg_temperature': round(avg_temp, 2),
                'optimal_temp_min': optimal_min,
                'optimal_temp_max': optimal_max
            })

    crop_df = pd.DataFrame(crop_effectiveness_data)
    print(f"‚úÖ Created crop-specific dataset: {crop_df.shape}")
    print(f"üìä Sample preview:")
    print(crop_df.head())

    return crop_df

"""## DECISION TREE ANALYSIS"""

def build_crop_effectiveness_decision_tree(crop_df):
    """Build decision tree for crop effectiveness analysis"""
    print("\nüå≥ Building Crop Effectiveness Decision Tree...")
    print("="*60)

    # Prepare features for decision tree
    feature_columns = ['current_production', 'temperature_suitability', 'avg_temperature',
                      'optimal_temp_min', 'optimal_temp_max']

    # Encode categorical variables
    le_crop = LabelEncoder()
    le_county = LabelEncoder()

    tree_data = crop_df.copy()
    tree_data['crop_encoded'] = le_crop.fit_transform(tree_data['crop'])
    tree_data['county_encoded'] = le_county.fit_transform(tree_data['county'])

    # Add encoded features to feature list
    feature_columns.extend(['crop_encoded', 'county_encoded'])

    X = tree_data[feature_columns].fillna(0)
    y = tree_data['effectiveness_level']

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train decision tree
    dt_model = DecisionTreeClassifier(
        max_depth=10,
        min_samples_split=20,
        min_samples_leaf=10,
        random_state=42
    )
    dt_model.fit(X_train, y_train)

    # Evaluate model
    y_pred = dt_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"‚úÖ Decision Tree Model Accuracy: {accuracy:.4f}")

    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_columns,
        'importance': dt_model.feature_importances_
    }).sort_values('importance', ascending=False)

    print("\nüìä Feature Importance:")
    for _, row in feature_importance.iterrows():
        print(f"   {row['feature']}: {row['importance']:.4f}")

    return dt_model, feature_importance, le_crop, le_county, accuracy

"""## SANKEY DIAGRAM VISUALIZATION"""

def create_crop_specific_sankey_diagram(crop_df):
    """Create Sankey diagram showing county -> crop -> effectiveness flows (limited to top 10 counties)"""
    print("\nüåä Creating County ‚Üí Crop ‚Üí Effectiveness Sankey Diagram (Top 10 Counties)...")
    print("="*60)

    # Select top 10 counties by total production
    top_counties_by_production = crop_df.groupby('county')['current_production'].sum().nlargest(10)
    top_counties = top_counties_by_production.index.tolist()

    print(f"üìä Selected top 10 counties by production: {', '.join(top_counties[:5])}...")

    # Filter data to include only top 10 counties
    filtered_crop_df = crop_df[crop_df['county'].isin(top_counties)].copy()

    # Prepare data for Sankey
    counties = filtered_crop_df['county'].unique()
    crops = filtered_crop_df['crop'].unique()
    effectiveness_levels = filtered_crop_df['effectiveness_level'].unique()

    # Create all unique nodes
    county_nodes = [f"{county}" for county in counties]
    crop_nodes = [f"{crop}" for crop in crops]
    effectiveness_nodes = [f"{level} Effectiveness" for level in effectiveness_levels]

    all_nodes = county_nodes + crop_nodes + effectiveness_nodes

    # Create flow data
    source_indices = []
    target_indices = []
    flow_values = []

    # County -> Crop flows
    for _, row in filtered_crop_df.iterrows():
        county_idx = county_nodes.index(row['county'])
        crop_idx = len(county_nodes) + crop_nodes.index(row['crop'])
        flow_weight = max(1, row['current_production'] / 100)

        source_indices.append(county_idx)
        target_indices.append(crop_idx)
        flow_values.append(flow_weight)

    # Crop -> Effectiveness flows
    crop_effectiveness_summary = filtered_crop_df.groupby(['crop', 'effectiveness_level']).agg({
        'current_production': 'sum',
        'effectiveness_ratio': 'mean'
    }).reset_index()

    for _, row in crop_effectiveness_summary.iterrows():
        crop_idx = len(county_nodes) + crop_nodes.index(row['crop'])
        effectiveness_idx = len(county_nodes) + len(crop_nodes) + effectiveness_nodes.index(f"{row['effectiveness_level']} Effectiveness")
        flow_weight = max(1, row['current_production'] / 100)

        source_indices.append(crop_idx)
        target_indices.append(effectiveness_idx)
        flow_values.append(flow_weight)

    # Define colors
    effectiveness_colors = {
        'High': '#2E8B57',    # Green
        'Moderate': '#FFD700', # Yellow
        'Low': '#DC143C'      # Red
    }

    # Node colors
    node_colors = (['#87CEEB'] * len(county_nodes) +  # Light blue for counties
                   ['#DDA0DD'] * len(crop_nodes) +     # Plum for crops
                   [effectiveness_colors[level.split()[0]] for level in effectiveness_nodes])

    # Link colors
    link_colors = []
    for i, (source, target) in enumerate(zip(source_indices, target_indices)):
        if target >= len(county_nodes) + len(crop_nodes):  # Effectiveness level
            level_name = all_nodes[target].split()[0]
            color = effectiveness_colors[level_name]
            if color == '#2E8B57':
                link_colors.append('rgba(46, 139, 87, 0.6)')
            elif color == '#FFD700':
                link_colors.append('rgba(255, 215, 0, 0.6)')
            else:
                link_colors.append('rgba(220, 20, 60, 0.6)')
        else:
            link_colors.append('rgba(135, 206, 235, 0.4)')

    # Create Sankey diagram
    fig = go.Figure(data=[go.Sankey(
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=all_nodes,
            color=node_colors
        ),
        link=dict(
            source=source_indices,
            target=target_indices,
            value=flow_values,
            color=link_colors
        )
    )])

    fig.update_layout(
        title="Top 10 Counties ‚Üí Crop ‚Üí Effectiveness Flow Analysis<br><sub>Green: High Effectiveness | Yellow: Moderate | Red: Low</sub>",
        font_size=12,
        width=1400,
        height=800
    )

    fig.show()

    # Print statistics
    print(f"\nüìä Sankey Flow Statistics (Top 10 Counties):")
    print("-" * 50)
    for level in effectiveness_levels:
        level_data = filtered_crop_df[filtered_crop_df['effectiveness_level'] == level]
        county_count = level_data['county'].nunique()
        crop_count = level_data['crop'].nunique()
        total_flow = level_data['current_production'].sum()
        print(f"{level} Effectiveness:")
        print(f"  Counties: {county_count}")
        print(f"  Crops: {crop_count}")
        print(f"  Total Production: {total_flow:,.0f}")

    return fig

"""## COMPREHENSIVE ANALYSIS AND VISUALIZATION"""

def display_comprehensive_analysis(crop_df):
    """Display comprehensive crop effectiveness analysis"""
    print("\nüìä COMPREHENSIVE CROP EFFECTIVENESS ANALYSIS")
    print("="*80)

    # Overall statistics
    total_combinations = len(crop_df)
    high_eff = len(crop_df[crop_df['effectiveness_level'] == 'High'])
    moderate_eff = len(crop_df[crop_df['effectiveness_level'] == 'Moderate'])
    low_eff = len(crop_df[crop_df['effectiveness_level'] == 'Low'])

    print(f"üìà Overall Statistics:")
    print(f"   Total county-crop combinations: {total_combinations}")
    print(f"   High effectiveness: {high_eff} ({high_eff/total_combinations*100:.1f}%)")
    print(f"   Moderate effectiveness: {moderate_eff} ({moderate_eff/total_combinations*100:.1f}%)")
    print(f"   Low effectiveness: {low_eff} ({low_eff/total_combinations*100:.1f}%)")

    # Top performers
    top_performers = crop_df.nlargest(20, 'effectiveness_ratio')
    print(f"\nüèÜ Top 20 Performing County-Crop Combinations:")
    print("-" * 60)
    for _, row in top_performers.iterrows():
        print(f"   {row['county']:<20} | {row['crop']:<15} | Ratio: {row['effectiveness_ratio']:.3f} | Level: {row['effectiveness_level']}")

    # Effectiveness by crop
    crop_effectiveness = crop_df.groupby('crop')['effectiveness_level'].value_counts().unstack(fill_value=0)
    print(f"\nüåæ Effectiveness by Crop:")
    print(crop_effectiveness)

    # Effectiveness by county (top 10)
    county_effectiveness = crop_df.groupby('county')['effectiveness_level'].value_counts().unstack(fill_value=0)
    top_counties = county_effectiveness.sum(axis=1).nlargest(10)
    print(f"\nüèòÔ∏è Top 10 Counties by Total Combinations:")
    print(county_effectiveness.loc[top_counties.index])

    return crop_df

def create_enhanced_visualizations(crop_df):
    """Create enhanced visualizations for crop effectiveness analysis"""
    print("\nüìä Creating Enhanced Visualizations...")
    print("="*50)

    # Set up plotting style
    plt.style.use('default')
    color_map = {'High': '#2E8B57', 'Moderate': '#FFD700', 'Low': '#DC143C'}

    # 1. Overall effectiveness distribution
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Crop Effectiveness Analysis Dashboard', fontsize=16, fontweight='bold')

    # Effectiveness level distribution
    effectiveness_counts = crop_df['effectiveness_level'].value_counts()
    colors = [color_map[level] for level in effectiveness_counts.index]
    axes[0, 0].pie(effectiveness_counts.values, labels=effectiveness_counts.index,
                   autopct='%1.1f%%', colors=colors, startangle=90)
    axes[0, 0].set_title('Overall Effectiveness Distribution', fontsize=14, fontweight='bold')

    # Effectiveness by crop
    crop_eff = crop_df.groupby(['crop', 'effectiveness_level']).size().unstack(fill_value=0)
    crop_eff.plot(kind='bar', stacked=True, ax=axes[0, 1],
                  color=[color_map[col] for col in crop_eff.columns])
    axes[0, 1].set_title('Effectiveness by Crop', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('Crop')
    axes[0, 1].set_ylabel('Number of Counties')
    axes[0, 1].tick_params(axis='x', rotation=45)
    axes[0, 1].legend()

    # Temperature vs Effectiveness
    for level in ['High', 'Moderate', 'Low']:
        level_data = crop_df[crop_df['effectiveness_level'] == level]
        axes[1, 0].scatter(level_data['avg_temperature'], level_data['effectiveness_ratio'],
                          c=color_map[level], label=level, alpha=0.6, s=30)
    axes[1, 0].set_xlabel('Average Temperature (¬∞C)')
    axes[1, 0].set_ylabel('Effectiveness Ratio')
    axes[1, 0].set_title('Temperature vs Effectiveness', fontsize=14, fontweight='bold')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # Production distribution by effectiveness
    for level in ['High', 'Moderate', 'Low']:
        level_data = crop_df[crop_df['effectiveness_level'] == level]
        axes[1, 1].hist(level_data['current_production'], alpha=0.6,
                       label=level, color=color_map[level], bins=20)
    axes[1, 1].set_xlabel('Current Production')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].set_title('Production Distribution by Effectiveness', fontsize=14, fontweight='bold')
    axes[1, 1].legend()
    axes[1, 1].set_yscale('log')

    plt.tight_layout()
    plt.show()

    # 2. Correlation heatmap
    plt.figure(figsize=(12, 8))
    numeric_data = crop_df[['current_production', 'predicted_yield', 'actual_yield',
                           'effectiveness_ratio', 'temperature_suitability', 'avg_temperature']]
    correlation_matrix = numeric_data.corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu_r', center=0,
                square=True, linewidths=0.5)
    plt.title('Crop Effectiveness Metrics Correlation Matrix', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

    print("‚úÖ Enhanced visualizations created successfully!")

"""MAIN EXECUTION"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tabulate import tabulate
import plotly.graph_objects as go


def run_complete_analysis():
    """Execute the complete Random Forest crop effectiveness analysis"""
    print("\nüöÄ RUNNING COMPLETE RANDOM FOREST ANALYSIS")
    print("="*80)

    # Perform basic EDA for each dataframe
    print("\nüìä PERFORMING BASIC EDA:")
    for name, df in dataframes.items():
        if isinstance(df, dict):
            print(f"\n{name} - Excel file with multiple sheets:")
            for sheet_name, sheet_df in df.items():
                print(f"  Sheet: {sheet_name}")
                display_summary(sheet_df, f"{name}_{sheet_name}")
        else:
            perform_eda(df)

    # Step 1: Train environmental model
    env_model, feature_cols, sensor_data = train_environmental_suitability_model(dataframes)

    # Step 2: Create crop-specific dataset
    crop_df = create_crop_specific_dataset(dataframes, env_model, feature_cols)

    if crop_df is None:
        print("‚ùå Could not create crop dataset. Analysis cannot continue.")
        return

    # Step 3: Build decision tree
    dt_model, feature_importance, le_crop, le_county, dt_accuracy = build_crop_effectiveness_decision_tree(crop_df)

    # Step 4: Create Sankey diagram
    sankey_fig = create_crop_specific_sankey_diagram(crop_df)

    # Step 5: Display comprehensive analysis
    display_comprehensive_analysis(crop_df)

    # Step 6: Create enhanced visualizations
    create_enhanced_visualizations(crop_df)

    print(f"\nüéâ ANALYSIS COMPLETE!")
    print("="*50)
    print(f"üìä Final Summary:")
    print(f"   ‚Ä¢ Total crop-county combinations analyzed: {len(crop_df)}")
    print(f"   ‚Ä¢ Unique counties: {crop_df['county'].nunique()}")
    print(f"   ‚Ä¢ Unique crops/livestock: {crop_df['crop'].nunique()}")
    print(f"   ‚Ä¢ High effectiveness combinations: {len(crop_df[crop_df['effectiveness_level'] == 'High'])}")
    print(f"   ‚Ä¢ Decision tree accuracy: {dt_accuracy:.4f}")

    return crop_df, env_model, dt_model, sankey_fig

"""EXECUTE ANALYSIS"""

# Run the complete analysis
results = run_complete_analysis()
crop_df, env_model, dt_model, sankey_fig = results

# Display final results summary
print("\n‚úÖ GOOGLE COLAB RANDOM FOREST ANALYSIS COMPLETE!")
print("="*80)
print("üìä Key Outputs Generated:")
print("   ‚Ä¢ Environmental suitability Random Forest model")
print("   ‚Ä¢ Crop effectiveness dataset with 4,000+ combinations")
print("   ‚Ä¢ Decision tree classification model")
print("   ‚Ä¢ Interactive Sankey diagram (County ‚Üí Crop ‚Üí Effectiveness)")
print("   ‚Ä¢ Comprehensive visualizations and analysis")
print("   ‚Ä¢ Statistical summaries and insights")

"""#K-MEANS CLUSTERING"""

# Import libraries for data handling, clustering, and visualization
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go

# Assume 'dataframes' dictionary from EDA script with sensor069, sensor070, sensor071, crop_distribution

# Step 1: Prepare county-level data
crop_df = dataframes["crop_distribution"]
crop_df = crop_df.rename(columns={"County/Sub County": "County"})
crop_df["County"] = crop_df["County"].str.strip().str.upper()
kenyan_counties = ["Mombasa", "Kwale", "Kilifi", "Tana River", "Lamu", "Taita Taveta", "Garissa", "Wajir", "Mandera", "Marsabit", "Isiolo", "Meru", "Tharaka Nithi", "Embu", "Kitui", "Machakos", "Makueni", "Nyandarua", "Nyeri", "Kirinyaga", "Murang'a", "Kiambu", "Turkana", "West Pokot", "Samburu", "Trans Nzoia", "Uasin Gishu", "Elgeyo Marakwet", "Nandi", "Baringo", "Laikipia", "Nakuru", "Narok", "Kajiado", "Kericho", "Bomet", "Kakamega", "Vihiga", "Bungoma", "Busia", "Siaya", "Kisumu", "Homa Bay", "Migori", "Kisii", "Nyamira", "Nairobi"]
kenyan_counties_upper = [c.upper() for c in kenyan_counties]
crop_df["IsCounty"] = crop_df["County"].isin(kenyan_counties_upper)
county_crop_df = crop_df[crop_df["IsCounty"]].copy()
numeric_cols = county_crop_df.select_dtypes(include=[np.number]).columns
county_crop_df = county_crop_df.groupby("County")[numeric_cols].sum().reset_index()

# Step 2: Select features and filter bottom 10 counties by data availability
feature_columns = ["Exotic cattle 0Dairy", "Exotic cattle 0Beef", "Indigenous cattle", "Sheep", "Goats", "Camels", "Donkeys", "Pigs", "Indigenous Chicken", "Exotic Chicken Layers", "Exotic Chicken Broilers", "Beehives", "Rabbits", "Fish Ponds", "Fish Cage"]
available_feature_columns = [col for col in feature_columns if col in county_crop_df.columns]
if not available_feature_columns: raise ValueError("No valid features")
feature_columns = available_feature_columns
county_crop_df = county_crop_df[["County"] + feature_columns].fillna(0)

livestock_columns = feature_columns
crop_columns = []

county_crop_df["Total_Households"] = county_crop_df[livestock_columns].sum(axis=1)

# Filter bottom 10 counties based on Total_Households
low_data_df = county_crop_df.nsmallest(20, "Total_Households").copy()

# Step 3: Aggregate weather data
sensor069_df = dataframes["sensor069"]
sensor070_df = dataframes["sensor070"]
sensor071_df = dataframes["sensor071"]
sensor069_df["date"] = pd.to_datetime(sensor069_df["date"])
sensor070_df["date"] = pd.to_datetime(sensor070_df["date"])
sensor071_df["date"] = pd.to_datetime(sensor071_df["date"])
temp_2022 = sensor069_df[sensor069_df["date"].dt.year == 2022]["temperature"].mean()
humidity_2022 = sensor070_df[sensor070_df["date"].dt.year == 2022]["relativeHumidity"].mean()
pressure_2022 = sensor071_df[sensor071_df["date"].dt.year == 2022]["pressure"].mean()
weather_df = pd.DataFrame({"County": kenyan_counties_upper, "Avg_Temperature": temp_2022, "Avg_Humidity": humidity_2022, "Avg_Pressure": pressure_2022})

# Step 4: Combine data
final_df = low_data_df.merge(weather_df, on="County", how="left")
final_df = final_df.fillna({"Avg_Temperature": temp_2022, "Avg_Humidity": humidity_2022, "Avg_Pressure": pressure_2022})

# Step 5: Calculate effectiveness
livestock_effectiveness = final_df[livestock_columns].sum(axis=1)
livestock_effectiveness = (livestock_effectiveness / livestock_effectiveness.max() * 100).round(2)
final_df["Livestock_Effectiveness"] = livestock_effectiveness
final_df["Crop_Effectiveness"] = 0

# Step 6: Hypertune feature selection and scaling
feature_sets = {
    "Livestock Only": livestock_columns,
    "Livestock + Weather": livestock_columns + ["Avg_Temperature", "Avg_Humidity", "Avg_Pressure"]
}
scalers = {"StandardScaler": StandardScaler(), "MinMaxScaler": MinMaxScaler(), "RobustScaler": RobustScaler()}
best_score = -1; best_feature_set = None; best_features = None; best_scaler_name = None
for scaler_name, scaler in scalers.items():
    for name, features in feature_sets.items():
        X = final_df[features].copy().fillna(0)
        weights = final_df["Total_Households"] / final_df["Total_Households"].sum()
        X_weighted = X.mul(weights, axis=0)
        X_scaled = scaler.fit_transform(X_weighted)
        kmeans = KMeans(n_clusters=3, random_state=42); kmeans.fit(X_scaled)
        silhouette = silhouette_score(X_scaled, kmeans.labels_)
        if silhouette > best_score:
            best_score = silhouette; best_feature_set = name; best_features = features; best_scaler_name = scaler_name

# Step 7: Scale features with best config
scaler = scalers[best_scaler_name]
X = final_df[best_features].copy().fillna(0)
weights = final_df["Total_Households"] / final_df["Total_Households"].sum()
X_weighted = X.mul(weights, axis=0)
X_scaled = scaler.fit_transform(X_weighted)

# Step 8: Cluster with k=3
optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
final_df["Cluster"] = kmeans.fit_predict(X_scaled)

# Step 9: PCA for 2D Visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
final_df["PCA1"] = X_pca[:, 0]
final_df["PCA2"] = X_pca[:, 1]
plt.figure(figsize=(10, 7))
sns.scatterplot(data=final_df, x="PCA1", y="PCA2", hue="Cluster", palette="deep", s=100)
plt.title("Bottom 10 County Clusters (Livestock Focus)")
plt.show()

# Step 10: Cluster Summary
cluster_summary = final_df.groupby("Cluster")[livestock_columns + ["Livestock_Effectiveness"]].mean().round(2)
print("\nCluster Summary:")
print(cluster_summary)

# Step 11: Best/Moderate/Least livestock per cluster
predictions_data = []
for cluster in range(optimal_k):
    cluster_df = final_df[final_df["Cluster"] == cluster]
    livestock_means = cluster_df[livestock_columns].mean().sort_values(ascending=False)
    if len(livestock_means) >= 3:
        predictions_data.extend([
            {"cluster": cluster, "category": "Best Livestock", "type": livestock_means.index[0], "value": livestock_means.iloc[0]},
            {"cluster": cluster, "category": "Moderate Livestock", "type": livestock_means.index[1], "value": livestock_means.iloc[1]},
            {"cluster": cluster, "category": "Least Livestock", "type": livestock_means.index[-1], "value": livestock_means.iloc[-1]}
        ])
predictions_df = pd.DataFrame(predictions_data)

# Step 12: Sankey Diagram (bottom 20 counties)
node_labels = final_df["County"].tolist()
for _, row in predictions_df.iterrows():
    node_labels.append(f"{row['type']} ({row['category'].split()[0]})")
node_colors = ["#1f77b4"] * len(final_df)
for _, row in predictions_df.iterrows():
    category = row["category"].split()[0]
    color = {"Best": "#00FF00", "Moderate": "#FFFF00", "Least": "#FF0000"}[category]
    node_colors.append(color)
source = []; target = []; value = []; link_colors = []
for _, row in final_df.iterrows():
    county = row["County"]; cluster = row["Cluster"]
    county_idx = node_labels.index(county)
    cluster_preds = predictions_df[predictions_df["cluster"] == cluster]
    for _, pred in cluster_preds.iterrows():
        target_label = f"{pred['type']} ({pred['category'].split()[0]})"
        target_idx = node_labels.index(target_label)
        type_value = row[pred["type"]] if pred["type"] in final_df.columns else 0
        if type_value > 0:
            source.append(county_idx); target.append(target_idx); value.append(type_value)
            link_color = {"Best": "rgba(0, 255, 0, 0.4)", "Moderate": "rgba(255, 255, 0, 0.4)", "Least": "rgba(255, 0, 0, 0.4)"}[pred["category"].split()[0]]
            link_colors.append(link_color)
fig = go.Figure(data=[go.Sankey(
    node=dict(pad=15, thickness=20, line=dict(color="black", width=0.5), label=node_labels, color=node_colors),
    link=dict(source=source, target=target, value=value, color=link_colors)
)])
fig.update_layout(title_text="Bottom 20 Counties to Best/Moderate/Least Livestock (k=3)", font_size=12)
fig.show()

# Step 13: Print relative clustering info
print("\nBottom 10 Counties Cluster Assignment:")
print(final_df[["County", "Cluster", "Livestock_Effectiveness"]].sort_values("Livestock_Effectiveness"))

# Step 14: Save results
final_df[["County", "Cluster"] + best_features + ["Livestock_Effectiveness"]].to_csv("bottom10_county_clusters_livestock.csv", index=False)
predictions_df[["cluster", "category", "type", "value"]].to_csv("bottom10_best_moderate_least_livestock.csv", index=False)

"""## K-MEANS"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, random_split
import numpy as np


# Convert to tensors
X_tensor = torch.tensor(X_resampled, dtype=torch.float32)
y_tensor = torch.tensor(y_resampled.values, dtype=torch.long)

# Dataset
dataset = TensorDataset(X_tensor, y_tensor)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))
train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=8)

# SIMPLER MODEL to prevent overfitting
class SimpleNet(nn.Module):
    def __init__(self, input_size, output_size):
        super(SimpleNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, 16),
            nn.ReLU(),
            nn.Dropout(0.4),  # stronger dropout to reduce overfitting
            nn.Linear(16, output_size)
        )

    def forward(self, x):
        return self.net(x)

model = SimpleNet(X_tensor.shape[1], len(np.unique(y_resampled)))

# Loss and optimizer with stronger weight decay
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-2)  # stronger L2

# Train for fewer epochs
epochs = 15
best_val_acc = 0
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for xb, yb in train_loader:
        optimizer.zero_grad()
        preds = model(xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    model.eval()
    correct, total, val_loss = 0, 0, 0
    with torch.no_grad():
        for xb, yb in val_loader:
            preds = model(xb)
            val_loss += criterion(preds, yb).item()
            correct += (preds.argmax(1) == yb).sum().item()
            total += yb.size(0)
    val_acc = correct / total
    best_val_acc = max(best_val_acc, val_acc)
    print(f"Epoch {epoch+1}/{epochs} | Train Loss: {total_loss:.3f} | Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.2f}")

print(f"\n Final Validation Accuracy: {val_acc * 100:.2f}%")

"""#K-MEANS MODEL"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Step 1: Prepare features and labels
X = final_df[livestock_columns + ["Avg_Temperature", "Avg_Humidity", "Avg_Pressure"]].copy()
y = final_df["Cluster"].copy()  # using previous KMeans cluster

# Step 2: Filter for clusters with at least 2 samples (needed for SMOTE)
valid_clusters = y.value_counts()[y.value_counts() >= 2].index
filtered_df = final_df[final_df["Cluster"].isin(valid_clusters)].copy()
X = filtered_df[livestock_columns + ["Avg_Temperature", "Avg_Humidity", "Avg_Pressure"]]
y = filtered_df["Cluster"]

# Step 3: Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 4: Apply SMOTE to balance cluster classes
min_class_size = y.value_counts().min()
safe_k = max(1, min(5, min_class_size - 1))  # auto-adjust k_neighbors
smote = SMOTE(random_state=42, k_neighbors=safe_k)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

print("Resampled Cluster Distribution:")
print(pd.Series(y_resampled).value_counts())

# Step 5: Hyperparameter tuning for KNN
param_grid = {
    "n_neighbors": list(range(1, min(10, len(X_resampled)))),
    "weights": ["uniform", "distance"],
    "metric": ["euclidean", "manhattan"]
}
grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring="accuracy")
grid.fit(X_resampled, y_resampled)

print("Best Parameters:", grid.best_params_)
print("Best Cross-Validation Accuracy:", round(grid.best_score_, 4))

# Step 6: Predict KNN clusters for all 20 counties
X_all = final_df[livestock_columns + ["Avg_Temperature", "Avg_Humidity", "Avg_Pressure"]]
X_all_scaled = scaler.transform(X_all)
final_df["KNN_Cluster"] = grid.best_estimator_.predict(X_all_scaled)

# Step 7: Update reference cluster to KNN
final_df["Original_KMeans_Cluster"] = final_df["Cluster"]  # optional backup
final_df["Cluster"] = final_df["KNN_Cluster"]
del final_df["KNN_Cluster"]

# Step 8: PCA visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_all_scaled)
final_df["PCA1"], final_df["PCA2"] = X_pca[:, 0], X_pca[:, 1]

plt.figure(figsize=(10, 7))
sns.scatterplot(data=final_df, x="PCA1", y="PCA2", hue="Cluster", style="Original_KMeans_Cluster", palette="tab10", s=100)
plt.title("Final KNN Clustering (Replaced KMeans)")
plt.show()

# Step 9: Evaluation
print("\nClassification Report (Original KMeans vs Final KNN Cluster):")
print(classification_report(final_df["Original_KMeans_Cluster"], final_df["Cluster"]))

cm = confusion_matrix(final_df["Original_KMeans_Cluster"], final_df["Cluster"])
ConfusionMatrixDisplay(cm).plot(cmap='Blues')
plt.title("Confusion Matrix: Original KMeans vs Final KNN")
plt.show()

# Step 10: Cluster comparison summary
print("\nCluster Comparison Summary:")
print(final_df[["County", "Cluster", "Original_KMeans_Cluster", "Livestock_Effectiveness"]].sort_values("Livestock_Effectiveness"))